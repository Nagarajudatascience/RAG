
descrption:

An AI-powered assistant that uses LangChain, CrewAI agents, and RAG pipeline to answer questions from legal documents (PDFs, contracts, case law, etc.)

âœ… Use Case
Upload legal documents

Extract, chunk, and vectorize them

Ask questions like "What is the penalty clause in this contract?"

Multi-agent system answers with citations


ğŸ”§ Tech Stack
FastAPI â€“ API framework

CrewAI â€“ Agent manager

LangChain â€“ RAG & Prompting

ChromaDB / Pinecone â€“ Vector storage

Pydantic â€“ Validation

PyPDF / LangChain doc loaders â€“ File parsing

OpenAI / Claude â€“ LLMs


ğŸ§  Agents Overview

Agent	                       Role

DocumentAgent	              Ingests, chunks, embeds uploaded files
QueryAgent	               Receives user questions
RetrieverAgent	           Uses vector DB to fetch relevant chunks
AnswerAgent	               Uses RAG chain to compose final answer









ğŸ“ Directory Structure
css
Copy
Edit
legal_doc_ai/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ document_agent.py
â”‚   â”œâ”€â”€ query_agent.py
â”‚   â”œâ”€â”€ retriever_agent.py
â”‚   â””â”€â”€ answer_agent.py
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ vectorstore/
â”‚   â”œâ”€â”€ chromadb_handler.py
â”‚   â””â”€â”€ chunking.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ pdf_loader.py
â””â”€â”€ requirements.txt
ğŸ” Sample Code (Highlights Only)


ğŸ“„ agents/document_agent.py

from utils.pdf_loader import load_pdf
from vectorstore.chunking import chunk_texts, embed_and_store

def handle_upload(file_path: str):
    texts = load_pdf(file_path)
    chunks = chunk_texts(texts)
    embed_and_store(chunks)
    return {"status": "indexed"}


ğŸ§  agents/query_agent.py

from agents.retriever_agent import retrieve_context
from agents.answer_agent import generate_answer

def process_user_query(question: str):
    context = retrieve_context(question)
    return generate_answer(question, context)

ğŸ§  agents/retriever_agent.py

from vectorstore.chromadb_handler import get_relevant_docs

def retrieve_context(query: str):
    return get_relevant_docs(query)

ğŸ§  agents/answer_agent.py

from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

def generate_answer(query: str, context: str):
    prompt = f"Context: {context}\n\nQuestion: {query}"
    llm = OpenAI(temperature=0)
    return llm(prompt)

ğŸ“¤ utils/pdf_loader.py

from langchain.document_loaders import PyPDFLoader

def load_pdf(file_path: str):
    loader = PyPDFLoader(file_path)
    return loader.load()

ğŸ§  vectorstore/chromadb_handler.py

import chromadb
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

def embed_and_store(chunks: list):
    vectorstore = Chroma(persist_directory="./chroma", embedding_function=OpenAIEmbeddings())
    vectorstore.add_texts(chunks)

def get_relevant_docs(query: str):
    vectorstore = Chroma(persist_directory="./chroma", embedding_function=OpenAIEmbeddings())
    docs = vectorstore.similarity_search(query)
    return " ".join([doc.page_content for doc in docs])

ğŸš€ API Entry: api/main.py

from fastapi import FastAPI, UploadFile
from agents.document_agent import handle_upload
from agents.query_agent import process_user_query

app = FastAPI()

@app.post("/upload/")
async def upload_doc(file: UploadFile):
    with open(f"./{file.filename}", "wb") as f:
        f.write(await file.read())
    return handle_upload(f"./{file.filename}")

@app.post("/ask/")
async def ask_question(q: str):
    return {"answer": process_user_query(q)}

ğŸ“¦ requirements.txt
nginx
Copy
Edit
fastapi
uvicorn
langchain
openai
chromadb
pydantic
crewai
PyPDF2
